{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087a2047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Cannot load fast pose extraction, switched to legacy slow implementation. ####\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from modules.input_reader import VideoReader, ImageReader\n",
    "from modules.draw import Plotter3d, draw_poses\n",
    "from modules.parse_poses import parse_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcdfbe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_poses(poses_3d, R, t):\n",
    "    R_inv = np.linalg.inv(R)\n",
    "    for pose_id in range(len(poses_3d)):\n",
    "        pose_3d = poses_3d[pose_id].reshape((-1, 4)).transpose()\n",
    "        pose_3d[0:3, :] = np.dot(R_inv, pose_3d[0:3, :] - t)\n",
    "        poses_3d[pose_id] = pose_3d.transpose().reshape(-1)\n",
    "\n",
    "    return poses_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff137a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.inference_engine_pytorch import InferenceEnginePyTorch\n",
    "net = InferenceEnginePyTorch(\"human-pose-estimation-3d.pth\", \"cuda\", use_tensorrt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b47ff4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m space_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m     24\u001b[0m mean_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frame_provider:\n\u001b[0;32m     26\u001b[0m     current_time \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mgetTickCount()\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\大创项目工作文件夹\\基于人体姿态估计的运动伤情分析监控\\Code\\github\\lightweight-human-pose-estimation-3d-demo.pytorch-master【demo可跑通】【单帧】\\lightweight-human-pose-estimation-3d-demo.pytorch-master\\modules\\input_reader.py:17\u001b[0m, in \u001b[0;36mImageReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m     16\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_names[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx], cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m cannot be read\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_names[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx]))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "canvas_3d = np.zeros((720, 1280, 3), dtype=np.uint8)\n",
    "plotter = Plotter3d(canvas_3d.shape[:2])\n",
    "canvas_3d_window_name = 'Canvas 3D'\n",
    "cv2.namedWindow(canvas_3d_window_name)\n",
    "cv2.setMouseCallback(canvas_3d_window_name, Plotter3d.mouse_callback)\n",
    "\n",
    "\n",
    "file_path = os.path.join('data', 'extrinsics.json')\n",
    "with open(file_path, 'r') as f:\n",
    "    extrinsics = json.load(f)\n",
    "R = np.array(extrinsics['R'], dtype=np.float32)\n",
    "t = np.array(extrinsics['t'], dtype=np.float32)\n",
    "\n",
    "frame_provider = ImageReader(\"1.jpg\")\n",
    "is_video = False\n",
    "\n",
    "base_height = 256\n",
    "fx = -1\n",
    "\n",
    "delay = 1\n",
    "esc_code = 27\n",
    "p_code = 112\n",
    "space_code = 32\n",
    "mean_time = 0\n",
    "for frame in frame_provider:\n",
    "    current_time = cv2.getTickCount()\n",
    "    if frame is None:\n",
    "        break\n",
    "    input_scale = base_height / frame.shape[0]\n",
    "    scaled_img = cv2.resize(frame, dsize=None, fx=input_scale, fy=input_scale)\n",
    "    scaled_img = scaled_img[:, 0:scaled_img.shape[1] - (scaled_img.shape[1] % stride)]  # better to pad, but cut out for demo\n",
    "    if fx < 0:  # Focal length is unknown\n",
    "        fx = np.float32(0.8 * frame.shape[1])\n",
    "\n",
    "    inference_result = net.infer(scaled_img)\n",
    "    poses_3d, poses_2d = parse_poses(inference_result, input_scale, stride, fx, is_video)\n",
    "    edges = []\n",
    "    if len(poses_3d):\n",
    "        poses_3d = rotate_poses(poses_3d, R, t)\n",
    "        poses_3d_copy = poses_3d.copy()\n",
    "        x = poses_3d_copy[:, 0::4]\n",
    "        y = poses_3d_copy[:, 1::4]\n",
    "        z = poses_3d_copy[:, 2::4]\n",
    "        poses_3d[:, 0::4], poses_3d[:, 1::4], poses_3d[:, 2::4] = -z, x, -y\n",
    "\n",
    "        poses_3d = poses_3d.reshape(poses_3d.shape[0], 19, -1)[:, :, 0:3]\n",
    "        edges = (Plotter3d.SKELETON_EDGES + 19 * np.arange(poses_3d.shape[0]).reshape((-1, 1, 1))).reshape((-1, 2))\n",
    "    plotter.plot(canvas_3d, poses_3d, edges)\n",
    "    cv2.imshow(canvas_3d_window_name, canvas_3d)\n",
    "\n",
    "    draw_poses(frame, poses_2d)\n",
    "    current_time = (cv2.getTickCount() - current_time) / cv2.getTickFrequency()\n",
    "    if mean_time == 0:\n",
    "        mean_time = current_time\n",
    "    else:\n",
    "        mean_time = mean_time * 0.95 + current_time * 0.05\n",
    "    cv2.putText(frame, 'FPS: {}'.format(int(1 / mean_time * 10) / 10), (40, 80), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255))\n",
    "    cv2.imshow('ICV 3D Human Pose Estimation', frame)\n",
    "\n",
    "    key = cv2.waitKey(delay)\n",
    "    if key == esc_code:\n",
    "        break\n",
    "    if key == p_code:\n",
    "        if delay == 1:\n",
    "            delay = 0\n",
    "        else:\n",
    "            delay = 1\n",
    "    if delay == 0 or not is_video:  # allow to rotate 3D canvas while on pause\n",
    "        key = 0\n",
    "        while (key != p_code and key != esc_code and key != space_code):\n",
    "            plotter.plot(canvas_3d, poses_3d, edges)\n",
    "            cv2.imshow(canvas_3d_window_name, canvas_3d)\n",
    "            key = cv2.waitKey(33)\n",
    "        if key == esc_code:\n",
    "            break\n",
    "        else:\n",
    "            delay = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57adc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -m MODEL [--video VIDEO] [-d DEVICE] [--use-openvino] [--use-tensorrt]\n",
      "                             [--images IMAGES [IMAGES ...]] [--height-size HEIGHT_SIZE]\n",
      "                             [--extrinsics-path EXTRINSICS_PATH] [--fx FX]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -m/--model\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harry\\anaconda3\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = ArgumentParser(description='Lightweight 3D human pose estimation demo. '\n",
    "                                        'Press esc to exit, \"p\" to (un)pause video or process next image.')\n",
    "    parser.add_argument('-m', '--model',\n",
    "                        help='Required. Path to checkpoint with a trained model '\n",
    "                             '(or an .xml file in case of OpenVINO inference).',\n",
    "                        type=str, required=True)\n",
    "    parser.add_argument('--video', help='Optional. Path to video file or camera id.', type=str, default='')\n",
    "    parser.add_argument('-d', '--device',\n",
    "                        help='Optional. Specify the target device to infer on: CPU or GPU. '\n",
    "                             'The demo will look for a suitable plugin for device specified '\n",
    "                             '(by default, it is GPU).',\n",
    "                        type=str, default='GPU')\n",
    "    parser.add_argument('--use-openvino',\n",
    "                        help='Optional. Run network with OpenVINO as inference engine. '\n",
    "                             'CPU, GPU, FPGA, HDDL or MYRIAD devices are supported.',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--use-tensorrt', help='Optional. Run network with TensorRT as inference engine.',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--images', help='Optional. Path to input image(s).', nargs='+', default='')\n",
    "    parser.add_argument('--height-size', help='Optional. Network input layer height size.', type=int, default=256)\n",
    "    parser.add_argument('--extrinsics-path',\n",
    "                        help='Optional. Path to file with camera extrinsics.',\n",
    "                        type=str, default=None)\n",
    "    parser.add_argument('--fx', type=np.float32, default=-1, help='Optional. Camera focal length.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.video == '' and args.images == '':\n",
    "        raise ValueError('Either --video or --image has to be provided')\n",
    "\n",
    "    stride = 8\n",
    "    if args.use_openvino:\n",
    "        from modules.inference_engine_openvino import InferenceEngineOpenVINO\n",
    "        net = InferenceEngineOpenVINO(args.model, args.device)\n",
    "    else:\n",
    "        from modules.inference_engine_pytorch import InferenceEnginePyTorch\n",
    "        net = InferenceEnginePyTorch(args.model, args.device, use_tensorrt=args.use_tensorrt)\n",
    "\n",
    "    canvas_3d = np.zeros((720, 1280, 3), dtype=np.uint8)\n",
    "    plotter = Plotter3d(canvas_3d.shape[:2])\n",
    "    canvas_3d_window_name = 'Canvas 3D'\n",
    "    cv2.namedWindow(canvas_3d_window_name)\n",
    "    cv2.setMouseCallback(canvas_3d_window_name, Plotter3d.mouse_callback)\n",
    "\n",
    "    file_path = args.extrinsics_path\n",
    "    if file_path is None:\n",
    "        file_path = os.path.join('data', 'extrinsics.json')\n",
    "    with open(file_path, 'r') as f:\n",
    "        extrinsics = json.load(f)\n",
    "    R = np.array(extrinsics['R'], dtype=np.float32)\n",
    "    t = np.array(extrinsics['t'], dtype=np.float32)\n",
    "\n",
    "    frame_provider = ImageReader(args.images)\n",
    "    is_video = False\n",
    "    if args.video != '':\n",
    "        frame_provider = VideoReader(args.video)\n",
    "        is_video = True\n",
    "    base_height = args.height_size\n",
    "    fx = args.fx\n",
    "\n",
    "    delay = 1\n",
    "    esc_code = 27\n",
    "    p_code = 112\n",
    "    space_code = 32\n",
    "    mean_time = 0\n",
    "    for frame in frame_provider:\n",
    "        current_time = cv2.getTickCount()\n",
    "        if frame is None:\n",
    "            break\n",
    "        input_scale = base_height / frame.shape[0]\n",
    "        scaled_img = cv2.resize(frame, dsize=None, fx=input_scale, fy=input_scale)\n",
    "        scaled_img = scaled_img[:, 0:scaled_img.shape[1] - (scaled_img.shape[1] % stride)]  # better to pad, but cut out for demo\n",
    "        if fx < 0:  # Focal length is unknown\n",
    "            fx = np.float32(0.8 * frame.shape[1])\n",
    "\n",
    "        inference_result = net.infer(scaled_img)\n",
    "        poses_3d, poses_2d = parse_poses(inference_result, input_scale, stride, fx, is_video)\n",
    "        edges = []\n",
    "        if len(poses_3d):\n",
    "            poses_3d = rotate_poses(poses_3d, R, t)\n",
    "            poses_3d_copy = poses_3d.copy()\n",
    "            x = poses_3d_copy[:, 0::4]\n",
    "            y = poses_3d_copy[:, 1::4]\n",
    "            z = poses_3d_copy[:, 2::4]\n",
    "            poses_3d[:, 0::4], poses_3d[:, 1::4], poses_3d[:, 2::4] = -z, x, -y\n",
    "\n",
    "            poses_3d = poses_3d.reshape(poses_3d.shape[0], 19, -1)[:, :, 0:3]\n",
    "            edges = (Plotter3d.SKELETON_EDGES + 19 * np.arange(poses_3d.shape[0]).reshape((-1, 1, 1))).reshape((-1, 2))\n",
    "        plotter.plot(canvas_3d, poses_3d, edges)\n",
    "        cv2.imshow(canvas_3d_window_name, canvas_3d)\n",
    "\n",
    "        draw_poses(frame, poses_2d)\n",
    "        current_time = (cv2.getTickCount() - current_time) / cv2.getTickFrequency()\n",
    "        if mean_time == 0:\n",
    "            mean_time = current_time\n",
    "        else:\n",
    "            mean_time = mean_time * 0.95 + current_time * 0.05\n",
    "        cv2.putText(frame, 'FPS: {}'.format(int(1 / mean_time * 10) / 10),\n",
    "                    (40, 80), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255))\n",
    "        cv2.imshow('ICV 3D Human Pose Estimation', frame)\n",
    "\n",
    "        key = cv2.waitKey(delay)\n",
    "        if key == esc_code:\n",
    "            break\n",
    "        if key == p_code:\n",
    "            if delay == 1:\n",
    "                delay = 0\n",
    "            else:\n",
    "                delay = 1\n",
    "        if delay == 0 or not is_video:  # allow to rotate 3D canvas while on pause\n",
    "            key = 0\n",
    "            while (key != p_code\n",
    "                   and key != esc_code\n",
    "                   and key != space_code):\n",
    "                plotter.plot(canvas_3d, poses_3d, edges)\n",
    "                cv2.imshow(canvas_3d_window_name, canvas_3d)\n",
    "                key = cv2.waitKey(33)\n",
    "            if key == esc_code:\n",
    "                break\n",
    "            else:\n",
    "                delay = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7fa37c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
